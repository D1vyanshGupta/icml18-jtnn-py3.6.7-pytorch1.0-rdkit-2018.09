{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convnets for molecule regression\n",
    "# All Molecules\n",
    "### Xavier Bresson\n",
    "###Â September 7 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notebook mode on local MacBook Pro :\n",
    "source activate deep_mol<br>\n",
    "jupyter notebook<br><br>\n",
    "conda + deep_mol :<br>\n",
    "curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh<br>\n",
    "chmod +x ~/miniconda.sh<br>\n",
    "./miniconda.sh<br>\n",
    "source ~/.bashrc<br>\n",
    "conda create -n deep_mol python=3.6 numpy mkl ipython jupyter scipy pandas matplotlib scikit-learn networkx<br>\n",
    "source activate deep_mol<br>\n",
    "conda install pytorch torchvision -c pytorch<br>\n",
    "conda install --channel https://conda.anaconda.org/rdkit rdkit seaborn<br>\n",
    "\n",
    "\n",
    "### notebook mode on GPU server :\n",
    "ssh deeplearn@172.21.32.181 <br>\n",
    "tmux new -s mol<br>\n",
    "tmux attach -t mol<br>\n",
    "source activate deep_mol<br>\n",
    "jupyter-notebook --no-browser --ip=0.0.0.0 --port=8888 --NotebookApp.token='deeplearning'<br>\n",
    "ssh -N -f -L localhost:8701:localhost:8888 deeplearn@172.21.32.181<br>\n",
    "http://localhost:8701\n",
    "\n",
    "### terminal mode on GPU server :\n",
    "ssh deeplearn@172.21.32.181<br>\n",
    "python 01_molecule_qm9_regression.py --max_epochs 200 --batch_size 50 --decay_rate 1.1 --learning_rate 0.001 --hidden_dim 200 --L 12 --server_id 181 --gpu_id 0<br>\n",
    "AND<br>\n",
    "bash script.sh\n",
    "\n",
    "### monitor on GPU server :\n",
    "ssh deeplearn@172.21.32.181 <br>\n",
    "nvidia-smi<br>\n",
    "top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb #pdb.set_trace()\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import sys\n",
    "import collections\n",
    "import os\n",
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# molecule toolkit\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Two modes:\n",
    "# notebook_mode = True  => development mode with notebook\n",
    "# notebook_mode = False => experiments mode with terminal and parser (no notebook)\n",
    "###############\n",
    "notebook_mode = True\n",
    "#notebook_mode = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Parser to runs code.py code from terminal\n",
    "# Note: parser cannot be used in a notebook => SystemExit: 2\n",
    "###############\n",
    "if notebook_mode == False:\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='mol_net')\n",
    "    parser.add_argument('-max_epochs','--max_epochs', type=int, default=100)\n",
    "    parser.add_argument('-batch_size','--batch_size', type=int, default=50)\n",
    "    parser.add_argument('-decay_rate','--decay_rate', type=float, default=1.25)\n",
    "    parser.add_argument('-learning_rate','--learning_rate', type=float, default=1e-3)\n",
    "    parser.add_argument('-hidden_dim','--hidden_dim', type=int, default=200)\n",
    "    parser.add_argument('-L','--L', type=int, default=6)\n",
    "    parser.add_argument('-resnet_type','--resnet_type', type=str, default='vanilla_resnet')\n",
    "    parser.add_argument('-dropout_rate', '--dropout_rate', type=float, default=0)\n",
    "    parser.add_argument('-gpu_id','--gpu_id', type=int, default=0)\n",
    "    parser.add_argument('-server_id','--server_id', type=int, default=181)\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    \n",
    "    # GPU id\n",
    "    gpu_id = args.gpu_id\n",
    "    server_id = args.server_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "# select GPU for notebook\n",
    "if notebook_mode == True:\n",
    "    gpu_id = 3  # select GPU id: 0,1,2,3\n",
    "    server_id = 181\n",
    "    \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    #torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    gpu_id = -1\n",
    "    server_id = -1\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    #torch.manual_seed(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "class bucket(object):\n",
    "    \"\"\"\n",
    "    INPUT: list of smiles corresponding to molecules of the SAME size (N atoms),\n",
    "           list of corresponding rdkit molecules \n",
    "           atom and bond dictionaries\n",
    "           \n",
    "    ATTRIBUTE: self.atom: Tensor of size bs x N     (atom composition of the molecule)\n",
    "               self.rep:  Tensor of size bs x N     (repeat feature) \n",
    "               self.bond: Tensor of size bs x N x N (adjacency matrix)\n",
    "               self.smile: list containing all the smiles \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smile_list, rdkitmol_list, atom_dict, bond_dict ):\n",
    "        \n",
    "        self.N= rdkitmol_list[0].GetNumAtoms()\n",
    "        self.bs = len(rdkitmol_list)\n",
    "        \n",
    "        self.atom  =  torch.zeros( self.bs , self.N).long()\n",
    "        self.rep   =  torch.zeros( self.bs , self.N).long()\n",
    "        self.bond  =  torch.zeros( self.bs , self.N, self.N).long()\n",
    "        self.smile =  smile_list\n",
    "        \n",
    "        for idx, mol in enumerate(rdkitmol_list):\n",
    "            \n",
    "            n = mol.GetNumAtoms()\n",
    "            if n != self.N:\n",
    "                print('ERROR: mol does not have right size')\n",
    "                \n",
    "            at,r,bd = rdkitMol2pytorchTensor(mol, atom_dict, bond_dict)\n",
    "            self.atom[idx]=at\n",
    "            self.rep[idx]=r\n",
    "            self.bond[idx]=bd\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.bs\n",
    "    \n",
    "    \n",
    "class bucket_helper(object):\n",
    "    \"\"\"\n",
    "    provide a mapping from bucket idx to size of molecule contained in the given bucket\n",
    "    \"\"\" \n",
    "    def __init__(self, data):\n",
    "        self.bucket2size=[]\n",
    "        self.size2bucket={}\n",
    "        self.num_molecules=[]\n",
    "\n",
    "        for idx,bucket in enumerate(data):\n",
    "            sz=bucket.N\n",
    "            self.bucket2size.append(sz)\n",
    "            self.size2bucket[sz]=idx\n",
    "            \n",
    "            \n",
    "    \n",
    "data_folder= 'datasets/dataQM9'\n",
    "\n",
    "pickle_in = open(data_folder + \"_processed/train.pickle\",\"rb\")\n",
    "data_train = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"_processed/test.pickle\",\"rb\")\n",
    "data_test = pickle.load(pickle_in)\n",
    "\n",
    "import dictionaries as dic\n",
    "\n",
    "pickle_in = open(data_folder + \"_processed/atom_dict.pickle\",\"rb\")\n",
    "atom_dict = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"_processed/bond_dict.pickle\",\"rb\")\n",
    "bond_dict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(data_folder + \"_processed/info_train.pickle\",\"rb\")\n",
    "info_train = pickle.load(pickle_in)\n",
    "pickle_in = open(data_folder + \"_processed/info_test.pickle\",\"rb\")\n",
    "info_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of molecule of size 1: \t 2\n",
      "number of molecule of size 2: \t 5\n",
      "number of molecule of size 3: \t 7\n",
      "number of molecule of size 4: \t 26\n",
      "number of molecule of size 5: \t 111\n",
      "number of molecule of size 6: \t 522\n",
      "number of molecule of size 7: \t 2639\n",
      "number of molecule of size 8: \t 15146\n",
      "number of molecule of size 9: \t 92373\n",
      "\n",
      "###########################\n",
      "\n",
      "number of molecule of size 4: \t 4\n",
      "number of molecule of size 5: \t 6\n",
      "number of molecule of size 6: \t 52\n",
      "number of molecule of size 7: \t 266\n",
      "number of molecule of size 8: \t 1365\n",
      "number of molecule of size 9: \t 8307\n"
     ]
    }
   ],
   "source": [
    "data = data_train\n",
    "for bucket in data:\n",
    "    print('number of molecule of size {}: \\t {}'.format(bucket.N, len(bucket)))\n",
    "    \n",
    "print('\\n###########################\\n')\n",
    "      \n",
    "data = data_test\n",
    "for bucket in data:\n",
    "    print('number of molecule of size {}: \\t {}'.format(bucket.N, len(bucket)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def keep_bucket_of_size(dataset,sz_to_keep):\n",
    "    new_dataset=[]\n",
    "    for bucket in dataset:\n",
    "        if bucket.N in sz_to_keep:\n",
    "            new_dataset.append(bucket)\n",
    "    return new_dataset\n",
    "\n",
    "sz_to_keep = [9] # [8,9] \n",
    "data_test  = keep_bucket_of_size(data_test,sz_to_keep)\n",
    "data_train = keep_bucket_of_size(data_train,sz_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([    2,     5,     7,    26,   111,   522,  2639, 15146, 92373])\n"
     ]
    }
   ],
   "source": [
    "def compute_bucket_stats(data):\n",
    "\n",
    "    size_of_mol_in_bucket=[]\n",
    "    num_mol_in_bucket=[]\n",
    "\n",
    "    for idx,bucket in enumerate(data):\n",
    "        size_of_mol_in_bucket.append(bucket.N)\n",
    "        num_mol_in_bucket.append(bucket.bs)\n",
    "\n",
    "    size_of_mol_in_bucket = torch.LongTensor(size_of_mol_in_bucket)\n",
    "    num_mol_in_bucket = torch.LongTensor(num_mol_in_bucket)\n",
    "\n",
    "    return size_of_mol_in_bucket, num_mol_in_bucket\n",
    "\n",
    "\n",
    "possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_train)\n",
    "print(possible_sizes)\n",
    "print(num_mol_per_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class sampler_class(object):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "        possible_sizes:   LongTensor containing the possible sizes\n",
    "        num_mol_per_bucket: LongTensor containing the number of molecule for each given bucket\n",
    "        bs: batch size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bs, possible_sizes, num_mol_per_bucket):  \n",
    "        \n",
    "        self.possible_sizes = possible_sizes\n",
    "        self.num_mol_per_bucket = num_mol_per_bucket\n",
    "        self.num_buckets = len(possible_sizes)\n",
    "        self.bs=bs\n",
    "        self.not_empty = True\n",
    "\n",
    "        \n",
    "        # compute the number of batches held in each bucket (note: this is a float)\n",
    "        self.num_remaining_batch_per_bucket = torch.floor( num_mol_per_bucket.float() / bs )\n",
    "        \n",
    "        # initialize the idx to zero for each bucket\n",
    "        self.mol_idx_in_bucket=torch.zeros( self.num_buckets ).long()\n",
    "        \n",
    "        \n",
    "    def get_bucket_idx_and_mol_idx(self):\n",
    "          \n",
    "        prob = self.num_remaining_batch_per_bucket / self.num_remaining_batch_per_bucket.sum()\n",
    "        \n",
    "        # choose one bucket at random, \n",
    "        # get num of atoms corresponding to this bucket\n",
    "        # and get the idx of the first molecule for the batch to be extracted\n",
    "        bucket_idx = np.random.choice( self.num_buckets  , p=prob.numpy() )\n",
    "        mol_idx = self.mol_idx_in_bucket[bucket_idx].item()\n",
    "        \n",
    "        # update the trackers\n",
    "        self.num_remaining_batch_per_bucket[bucket_idx] -= 1\n",
    "        self.mol_idx_in_bucket[bucket_idx] += self.bs\n",
    "\n",
    "        #     \n",
    "        if self.num_remaining_batch_per_bucket.sum().long() == 0:\n",
    "            self.not_empty=False\n",
    "            \n",
    "        return bucket_idx , mol_idx\n",
    "    \n",
    "    \n",
    "batch_size = 5\n",
    "sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "\n",
    "buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "print(buck_idx)\n",
    "print(mol_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_atoms': 4, 'nb_bonds': 5, 'max_atom_count': 9, 'output_dim': 1, 'hidden_dim': 50, 'L': 2, 'flag_resnet': 'he_resnet_1', 'dropout_rate': 0.1, 'dropout_rate_1': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "net_parameters = {}\n",
    "net_parameters['nb_atoms'] = len(atom_dict.idx2word)\n",
    "net_parameters['nb_bonds'] = len(bond_dict.idx2word)\n",
    "net_parameters['max_atom_count'] = data_train[-1].N\n",
    "net_parameters['output_dim'] = 1\n",
    "\n",
    "net_parameters['hidden_dim'] = 50 # debug\n",
    "net_parameters['L'] = 2           # debug\n",
    "net_parameters['flag_resnet'] = 'he_resnet_1'\n",
    "net_parameters['dropout_rate'] = 0.1\n",
    "net_parameters['dropout_rate_1'] = 0.2\n",
    "\n",
    "#net_parameters['hidden_dim'] = 200\n",
    "#net_parameters['L'] = 6 \n",
    "\n",
    "if notebook_mode == True:\n",
    "    print(net_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeNet_regression(\n",
      "  (molecule_encoder): molecule_encoder(\n",
      "    (atoms_embedding): Embedding(4, 50)\n",
      "    (bonds_embedding): Embedding(5, 50)\n",
      "    (convnet_layers): ModuleList(\n",
      "      (0): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.1)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (edges_to_vector): edges_to_vector(\n",
      "      (gate): edge_convnet_feat(\n",
      "        (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (W): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "      (A): Linear(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): mlp(\n",
      "    (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (V): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "nb_param= 39151\n",
      "torch.Size([5])\n",
      "tensor([-2.5286, -2.2525, -3.8818, -1.6326, -3.0751],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "###############          \n",
    "class molecule_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    molecule encoder class\n",
    "    size of input train_x_node : B x V\n",
    "    size of input train_x_edge : B x V x V\n",
    "    size of output z : B x H\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_atoms, nb_bonds, hidden_dim, L, resnet, drp, drp_1):        \n",
    "        super(molecule_encoder, self).__init__()\n",
    "        \n",
    "        # atoms embedding\n",
    "        self.atoms_embedding = nn.Embedding(nb_atoms, hidden_dim)\n",
    "\n",
    "        # bonds embedding\n",
    "        self.bonds_embedding = nn.Embedding(nb_bonds, hidden_dim) \n",
    "        \n",
    "        #changing convonet leyers according to resnet type\n",
    "        if (resnet != 'no_resnet' and resnet != 'vanilla_resnet' and resnet != 'vanilla_resnet_pre'):\n",
    "            L = L//2\n",
    "\n",
    "        # list of convnet layers\n",
    "        convnet_layers = [] \n",
    "        for layer in range(L):\n",
    "            convnet_layers.append(basic_convnet_layer(hidden_dim, resnet, drp, drp_1))\n",
    "        self.convnet_layers = nn.ModuleList(convnet_layers)\n",
    "\n",
    "        # edges to vector \n",
    "        self.edges_to_vector = edges_to_vector(hidden_dim)\n",
    "        \n",
    "        # class variables\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, train_x_node, train_x_edge):\n",
    "        \n",
    "        x = self.atoms_embedding(train_x_node) # B x V x H\n",
    "        e = self.bonds_embedding(train_x_edge) # B x V x V x H\n",
    "        for layer in range(self.L):\n",
    "            x,e = self.convnet_layers[layer](x,e) # B x V x H,  B x V x V x H\n",
    "        z = self.edges_to_vector(x,e) # B x H\n",
    "       \n",
    "        return z\n",
    "###############       \n",
    "        \n",
    "\n",
    "###############\n",
    "class basic_convnet_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    basic convnet class\n",
    "    size of input x : B x V x H\n",
    "    size of input e : B x V x V x H\n",
    "    size of output x_new : B x V x H\n",
    "    size of output e_new : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, resnet, drp, drp_1):        \n",
    "        super(basic_convnet_layer, self).__init__()\n",
    "        \n",
    "        self.node_convnet_feat = node_convnet_feat(hidden_dim)\n",
    "        self.edge_convnet_feat = edge_convnet_feat(hidden_dim)\n",
    "        self.bn_node = bn_node(hidden_dim)      \n",
    "        self.bn_edge = bn_edge(hidden_dim)    \n",
    "        \n",
    "        self.resnet = resnet\n",
    "        \n",
    "        #dropout \n",
    "        self.drop = nn.Dropout(p=drp)\n",
    "        self.drop_e = nn.Dropout(p=drp_1)\n",
    "        \n",
    "        if (resnet != 'no_resnet' and resnet != 'vanilla_resnet' and resnet != 'vanilla_resnet_pre'):\n",
    "            self.node_convnet_feat_2 = node_convnet_feat(hidden_dim)\n",
    "            self.edge_convnet_feat_2 = edge_convnet_feat(hidden_dim)\n",
    "            self.bn_node_2 = bn_node(hidden_dim)      \n",
    "            self.bn_edge_2 = bn_edge(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        \n",
    "        if self.resnet == 'no_resnet':\n",
    "        \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e_tmp = self.edge_convnet_feat(x_in, e_in) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_in, edge_gate)\n",
    "            e_tmp = self.bn_edge(e_tmp)\n",
    "            x_tmp = self.bn_node(x_tmp)\n",
    "            e_new = F.relu(e_tmp)\n",
    "            x_new = F.relu(x_tmp)\n",
    " \n",
    "            return x_new, e_new\n",
    "        \n",
    "        if self.resnet == 'vanilla_resnet':\n",
    "        \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e_tmp = self.edge_convnet_feat(x_in, e_in) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_in, edge_gate)\n",
    "            e_tmp = self.bn_edge(e_tmp)\n",
    "            x_tmp = self.bn_node(x_tmp)\n",
    "            e = F.relu(e_tmp)\n",
    "            x = F.relu(x_tmp)\n",
    "            e_new = e_in + e #vanila resnet\n",
    "            x_new = x_in + x #vanila resnet\n",
    "            \n",
    "            return x_new, e_new\n",
    "\n",
    "        if self.resnet == 'vanilla_resnet_pre':\n",
    "        \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e = self.bn_edge(e_in)\n",
    "            x = self.bn_node(x_in)\n",
    "            e_tmp = self.edge_convnet_feat(x, e) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x, edge_gate)\n",
    "            e = F.relu(e_tmp)\n",
    "            x = F.relu(x_tmp)\n",
    "            e_new = e_in + e #vanila resnet\n",
    "            x_new = x_in + x #vanila resnet\n",
    "            \n",
    "            return x_new, e_new\n",
    "\n",
    "        if self.resnet == 'he_resnet_1':\n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            \n",
    "            x_d = self.drop(x_in) #dropout inbetween\n",
    "            \n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_in) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e_tmp = self.bn_edge(e_tmp)\n",
    "            x_tmp = self.bn_node(x_tmp)\n",
    "            e = F.relu(e_tmp)\n",
    "            x = F.relu(x_tmp)\n",
    "            \n",
    "            e_int = e_in + e  #intermediate residual edge\n",
    "            \n",
    "            \n",
    "            \n",
    "            e_in_2 = e_int\n",
    "            x_in_2 = x\n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x_in_2, e_in_2)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x_in_2, edge_gate_2)\n",
    "            e_tmp_2 = self.bn_edge_2(e_tmp_2)\n",
    "            x_tmp_2 = self.bn_node_2(x_tmp_2)\n",
    "            e = F.relu(e_tmp_2)\n",
    "            x = F.relu(x_tmp_2)\n",
    "            e_new = e_in_2 + e\n",
    "            x_new = x_in + x\n",
    "            \n",
    "            return x_new, e_new\n",
    "\n",
    "\n",
    "        if self.resnet == 'he_resnet_1_pre':\n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e_d = self.bn_edge(e_in)\n",
    "            x_d = self.bn_node(x_in)\n",
    "            \n",
    "            x_d = self.drop(x_d) #dropout inbetween\n",
    "            \n",
    "            e_d = F.relu(e_d)\n",
    "            x_d = F.relu(x_d)\n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_d) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e = e_tmp\n",
    "            x = x_tmp\n",
    "            \n",
    "            e_int = e_in + e  #intermediate residual edge\n",
    "            \n",
    "            \n",
    "            \n",
    "            e_in_2 = e_int\n",
    "            x_in_2 = x\n",
    "            e = self.bn_edge_2(e_in_2)\n",
    "            x = self.bn_node_2(x_in_2)\n",
    "            e = F.relu(e)\n",
    "            x = F.relu(x)\n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x, e)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x, edge_gate_2)\n",
    "            e = e_tmp_2\n",
    "            x = x_tmp_2\n",
    "            e_new = e_in_2 + e\n",
    "            x_new = x_in + x\n",
    "            \n",
    "            return x_new, e_new\n",
    "        \n",
    "        \n",
    "        if self.resnet == 'he_resnet_2':\n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            \n",
    "            x_d = self.drop(x_in) #dropout inbetween\n",
    "            \n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_in) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e_tmp = self.bn_edge(e_tmp)\n",
    "            x_tmp = self.bn_node(x_tmp)\n",
    "            e = F.relu(e_tmp)\n",
    "            x = F.relu(x_tmp)\n",
    "            \n",
    "           \n",
    "            \n",
    "            e_in_2 = e\n",
    "            x_in_2 = x\n",
    "            \n",
    "            e_d = self.drop_e(e_in_2) #edge dropout\n",
    "            \n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x_in_2, e_d)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x_in_2, edge_gate_2)\n",
    "            e_tmp_2 = self.bn_edge_2(e_tmp_2)\n",
    "            x_tmp_2 = self.bn_node_2(x_tmp_2)\n",
    "            e = F.relu(e_tmp_2)\n",
    "            x = F.relu(x_tmp_2)\n",
    "            e_new = e_in + e\n",
    "            x_new = x_in + x\n",
    "            \n",
    "            return x_new, e_new\n",
    "        \n",
    "        \n",
    "        if self.resnet == 'he_resnet_2_pre':\n",
    "            \n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e_d = self.bn_edge(e_in)\n",
    "            x_d = self.bn_node(x_in)\n",
    "            \n",
    "            x_d = self.drop(x_d) #dropout inbetween\n",
    "            \n",
    "            e_d = F.relu(e_d)\n",
    "            x_d = F.relu(x_d)\n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_d) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e = e_tmp\n",
    "            x = x_tmp\n",
    "            \n",
    "            \n",
    "            e_in_2 = e\n",
    "            x_in_2 = x\n",
    "            e = self.bn_edge_2(e_in_2)\n",
    "            x = self.bn_node_2(x_in_2)\n",
    "            e = F.relu(e)\n",
    "            x = F.relu(x)\n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x, e)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x, edge_gate_2)\n",
    "            e = e_tmp_2\n",
    "            x = x_tmp_2\n",
    "            e_new = e_in + e\n",
    "            x_new = x_in + x\n",
    "            \n",
    "            \n",
    "            return x_new, e_new\n",
    "        \n",
    "        \n",
    "        if self.resnet == 'he_resnet_3':\n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            \n",
    "            x_d = self.drop(x_in) #dropout inbetween\n",
    "            \n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_in) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e_tmp = self.bn_edge(e_tmp)\n",
    "            x_tmp = self.bn_node(x_tmp)\n",
    "            e = F.relu(e_tmp)\n",
    "            x = F.relu(x_tmp)\n",
    "            \n",
    "            x_int = x_in + x  #intermediate residual edge\n",
    "            \n",
    "            \n",
    "            \n",
    "            e_in_2 = e\n",
    "            x_in_2 = x_int\n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x_in_2, e_in_2)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x_in_2, edge_gate_2)\n",
    "            e_tmp_2 = self.bn_edge_2(e_tmp_2)\n",
    "            x_tmp_2 = self.bn_node_2(x_tmp_2)\n",
    "            e = F.relu(e_tmp_2)\n",
    "            x = F.relu(x_tmp_2)\n",
    "            e_new = e_in + e\n",
    "            x_new = x_in_2 + x\n",
    "            \n",
    "            return x_new, e_new\n",
    "        \n",
    "        \n",
    "        if self.resnet == 'he_resnet_3_pre':\n",
    "            \n",
    "            e_in = e\n",
    "            x_in = x\n",
    "            e_d = self.bn_edge(e_in)\n",
    "            x_d = self.bn_node(x_in)\n",
    "            \n",
    "            x_d = self.drop(x_d) #dropout inbetween\n",
    "            \n",
    "            e_d = F.relu(e_d)\n",
    "            x_d = F.relu(x_d)\n",
    "            e_tmp = self.edge_convnet_feat(x_d, e_d) # B x V x V x H\n",
    "            edge_gate = F.sigmoid(e_tmp)\n",
    "            x_tmp = self.node_convnet_feat(x_d, edge_gate)\n",
    "            e = e_tmp\n",
    "            x = x_tmp\n",
    "            \n",
    "            x_int = x_in + x  #intermediate residual node\n",
    "            \n",
    "            e_in_2 = e\n",
    "            x_in_2 = x_int\n",
    "            e = self.bn_edge_2(e_in_2)\n",
    "            x = self.bn_node_2(x_in_2)\n",
    "            e = F.relu(e)\n",
    "            x = F.relu(x)\n",
    "            e_tmp_2 = self.edge_convnet_feat_2(x, e)\n",
    "            edge_gate_2 = F.sigmoid(e_tmp_2)\n",
    "            x_tmp_2 = self.node_convnet_feat_2(x, edge_gate_2)\n",
    "            e = e_tmp_2\n",
    "            x = x_tmp_2\n",
    "            e_new = e_in + e\n",
    "            x_new = x_in_2 + x\n",
    "            \n",
    "            return x_new, e_new\n",
    "            \n",
    "            \n",
    "###############\n",
    "\n",
    "\n",
    "###############\n",
    "class node_convnet_feat(nn.Module):\n",
    "    \"\"\"\n",
    "    convnet features for nodes\n",
    "    x_i = U*x_i +  sum_j gate_ij * (V*x_j)\n",
    "    size of input x : B x V x H\n",
    "    size of output edge_gate : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(node_convnet_feat, self).__init__()\n",
    "         \n",
    "        self.U  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate):\n",
    "        \n",
    "        Ux = self.U(x) # B x V x H \n",
    "        Vx = self.V(x) # B x V x H\n",
    "        Vx = Vx.unsqueeze(1) # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        gateVx = edge_gate* Vx # B x V x V x H\n",
    "        x_new = Ux + torch.sum( gateVx , dim=2) # B x V x H\n",
    "        \n",
    "        return x_new\n",
    "###############\n",
    "\n",
    "\n",
    "###############       \n",
    "class edge_convnet_feat(nn.Module):\n",
    "    \"\"\"\n",
    "    convnet features for edges\n",
    "    e_ij = U*e_ij + V*x_i + W*x_j\n",
    "    size of input x : B x V x H\n",
    "    size of output e : # B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(edge_convnet_feat, self).__init__()\n",
    "         \n",
    "        self.U  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.W  = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = self.W(x)\n",
    "        Vx = Vx.unsqueeze(2) # extend Vx from \"B x V x H\" to \"B x V x 1 x H\"\n",
    "        Wx = Wx.unsqueeze(1) # extend Wx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        e_new = Ue + Vx + Wx\n",
    "        \n",
    "        return e_new\n",
    "###############  \n",
    "\n",
    "\n",
    "###############\n",
    "class bn_node(nn.Module):\n",
    "    \"\"\"\n",
    "    batch normalization for nodes\n",
    "    size of input x : B x V x H\n",
    "    size of output x_bn : B x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(bn_node, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_trans = x.transpose(1,2).contiguous() # input must be of shape: B x H x V\n",
    "        x_trans_bn = self.bn(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1,2).contiguous() \n",
    "        \n",
    "        return x_bn\n",
    "###############\n",
    "\n",
    "\n",
    "###############\n",
    "class bn_edge(nn.Module):\n",
    "    \"\"\"\n",
    "    batch normalization for edges\n",
    "    size of input e : B x V x V x H\n",
    "    size of output e_bn : B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(bn_edge, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(hidden_dim) \n",
    "\n",
    "    def forward(self, e):\n",
    "        \n",
    "        e_trans = e.transpose(1,3).contiguous() # input must be of shape: B x H x V x V\n",
    "        e_trans_bn = self.bn(e_trans)\n",
    "        e_bn = e_trans_bn.transpose(1,3).contiguous() \n",
    "        \n",
    "        return e_bn\n",
    "###############\n",
    "\n",
    "\n",
    "###############\n",
    "class edges_to_vector(nn.Module):\n",
    "    \"\"\"\n",
    "    vector representation of all edges \n",
    "    z = sum_ij gate_ij * (A*e_ij)\n",
    "    where gate_ij = sigmoid(U*e_ij + V*x_i + W*x_j)\n",
    "    size of input x : B x V x H\n",
    "    size of output e : # B x V x V x H\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):        \n",
    "        super(edges_to_vector, self).__init__()\n",
    "        \n",
    "        self.gate = edge_convnet_feat(hidden_dim)\n",
    "        self.A = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \n",
    "        edge_gate = self.gate(x,e)\n",
    "        edge_gate = F.sigmoid(edge_gate)\n",
    "        Ae = self.A(e)\n",
    "        gateAe = edge_gate * Ae\n",
    "        # sum over all edges ij\n",
    "        z = torch.sum(gateAe, dim=1)\n",
    "        z = torch.sum(z, dim=1)\n",
    "             \n",
    "        return z\n",
    "###############\n",
    "        \n",
    "    \n",
    "###############\n",
    "class mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer perceptron class\n",
    "    size of input x : B x H\n",
    "    size of output y : B x 1\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim):        \n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        Ux = self.U(x) # B x H\n",
    "        y = F.relu(Ux) # B x H\n",
    "        y = self.V(y) # B x H\n",
    "        \n",
    "        return y\n",
    "###############    \n",
    "        \n",
    "        \n",
    "###############\n",
    "class MoleculeNet_regression(nn.Module):\n",
    "    \"\"\"\n",
    "    network for molecule regression\n",
    "    \"\"\"\n",
    "    def __init__(self, net_parameters):        \n",
    "        super(MoleculeNet_regression, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        nb_atoms = net_parameters['nb_atoms']\n",
    "        nb_bonds = net_parameters['nb_bonds']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        L = net_parameters['L']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        resnet = net_parameters['flag_resnet']\n",
    "        drp = net_parameters['dropout_rate']\n",
    "        drp_1 = net_parameters['dropout_rate_1']\n",
    "        #print(nb_atoms,nb_bonds,hidden_dim,output_dim,L)\n",
    "        \n",
    "        # molecule encoder: from molecule to vector representation (computed with graph convnet)\n",
    "        self.molecule_encoder = molecule_encoder(nb_atoms, nb_bonds, hidden_dim, L, resnet, drp, drp_1)\n",
    "        \n",
    "        # regression part\n",
    "        #output_dim = 1\n",
    "        self.mlp = mlp(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, train_x_node, train_x_edge):\n",
    "        \n",
    "        z = self.molecule_encoder(train_x_node, train_x_edge) # B x H\n",
    "        regression_value = self.mlp(z).squeeze() # B\n",
    "    \n",
    "        return regression_value\n",
    "    \n",
    "        \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.L1Loss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "       \n",
    "        \n",
    "    def chemical_accuracy(self, y, y_target):\n",
    "        \n",
    "        chemical_accuracy_LUMO = 0.043\n",
    "        MAE = F.l1_loss(y, y_target)\n",
    "        MAE /= chemical_accuracy_LUMO\n",
    "\n",
    "        return MAE \n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.Adam( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "###############  \n",
    "    \n",
    "    \n",
    "    \n",
    "if notebook_mode == True:\n",
    "\n",
    "    # instantiate the network\n",
    "    net = MoleculeNet_regression(net_parameters)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "\n",
    "\n",
    "    # number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('nb_param=',nb_param)\n",
    "\n",
    "\n",
    "    # forward\n",
    "    batch_size = 5\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "    train_x_node = Variable( torch.LongTensor(data_train[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_x_node_count = Variable( torch.LongTensor(data_train[buck_idx].rep[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_x_edge = Variable( torch.LongTensor(data_train[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "    train_y = Variable( torch.FloatTensor(data_train[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "\n",
    "    y = net.forward(train_x_node, train_x_edge) # B x 1\n",
    "    print(y.size())\n",
    "    print(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'max_epochs': 2, 'decay_rate': 1.25, 'batch_size': 50}\n",
      "tensor(3.0593, device='cuda:0', grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['learning_rate'] = 0.001\n",
    "opt_parameters['max_epochs'] = 200  \n",
    "opt_parameters['max_epochs'] = 2 ########### debug\n",
    "opt_parameters['decay_rate'] = 1.25\n",
    "opt_parameters['batch_size'] = 50\n",
    "\n",
    "\n",
    "if notebook_mode == True:\n",
    "\n",
    "    print(opt_parameters)\n",
    "\n",
    "    learning_rate = opt_parameters['learning_rate']\n",
    "    max_epochs = opt_parameters['max_epochs']\n",
    "    decay_rate = opt_parameters['decay_rate']\n",
    "\n",
    "\n",
    "    # compute loss \n",
    "    loss = net.loss(y,train_y)\n",
    "    print(loss)\n",
    "\n",
    "    \n",
    "    # define optimizer\n",
    "    lr = learning_rate\n",
    "    optimizer = net.update(lr) \n",
    "\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  \n",
    "def train_one_epoch(net,optimizer,opt_parameters):\n",
    "    \"\"\"\n",
    "    train one epoch\n",
    "    \"\"\"\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    batch_size = opt_parameters['batch_size']\n",
    "    \n",
    "    possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_train)\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    \n",
    "    shuffling_map = []\n",
    "    for bucket_size in list(num_mol_per_bucket):\n",
    "        shuffling_map.append(np.random.permutation(bucket_size.item()))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    while sampler.not_empty:\n",
    "        \n",
    "        # extract batch\n",
    "        buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "        x_node = Variable( torch.LongTensor(data_train[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        x_edge = Variable( torch.LongTensor(data_train[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        y_target = Variable( torch.FloatTensor(data_train[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        optimizer.zero_grad()\n",
    "        y = net.forward(x_node, x_edge) # B \n",
    "        loss = net.loss(y,y_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute loss, accuracy\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()    \n",
    "        running_acc += batch_size* net.chemical_accuracy(y,y_target).item()   \n",
    "        running_nb_batch += 1 # for intermediate result\n",
    "\n",
    "        # print intermediate result\n",
    "        if 2==1:\n",
    "            if running_nb_batch%100==0:\n",
    "                 print('{}: loss={}'.format(running_nb_data,running_loss/running_nb_data))\n",
    "               \n",
    "            \n",
    "    # loss and acc values for one epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    acc = running_acc/ running_nb_data\n",
    "    \n",
    "    return loss, acc\n",
    "###############  \n",
    "    \n",
    "\n",
    "if notebook_mode==True and gpu_id==-1:\n",
    "    train_loss, train_acc = train_one_epoch(net, optimizer, opt_parameters)\n",
    "    print(train_loss, train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  \n",
    "def evaluate_test_set(net, opt_parameters):\n",
    "    \"\"\"\n",
    "    evaluate test set\n",
    "    \"\"\"\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    batch_size = opt_parameters['batch_size'] \n",
    "    \n",
    "    possible_sizes, num_mol_per_bucket = compute_bucket_stats(data_test)\n",
    "    sampler = sampler_class(batch_size, possible_sizes, num_mol_per_bucket)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_nb_data = 0\n",
    "    \n",
    "    while sampler.not_empty:\n",
    "        \n",
    "        # extract batch\n",
    "        buck_idx, mol_idx = sampler.get_bucket_idx_and_mol_idx()\n",
    "        x_node = Variable( torch.LongTensor(data_test[buck_idx].atom[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        x_edge = Variable( torch.LongTensor(data_test[buck_idx].bond[mol_idx:mol_idx+batch_size]).type(dtypeLong) , requires_grad=False)\n",
    "        y_target = Variable( torch.FloatTensor(data_test[buck_idx].lumo[mol_idx:mol_idx+batch_size]).type(dtypeFloat) , requires_grad=False)\n",
    "    \n",
    "        # forward, backward, optimize\n",
    "        y = net.forward(x_node, x_edge) # B \n",
    "        loss = net.loss(y,y_target)\n",
    "        \n",
    "        # compute loss, accuracy\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()    \n",
    "        running_acc += batch_size* net.chemical_accuracy(y,y_target).item()  \n",
    "\n",
    "        \n",
    "    # loss and acc values for one epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    acc = running_acc/ running_nb_data\n",
    "    \n",
    "    return loss, acc\n",
    "###############  \n",
    "    \n",
    "    \n",
    "if notebook_mode==True and gpu_id==-1:\n",
    "    test_loss, test_acc = evaluate_test_set(net, opt_parameters)\n",
    "    print(test_loss, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# parser values before running main()\n",
    "###############\n",
    "if notebook_mode == False:\n",
    "\n",
    "    # network parameters\n",
    "    net_parameters = {}\n",
    "    net_parameters['nb_atoms'] = len(atom_dict.idx2word)\n",
    "    net_parameters['nb_bonds'] = len(bond_dict.idx2word)\n",
    "    net_parameters['max_atom_count'] = data_train[-1].N\n",
    "    net_parameters['hidden_dim'] = args.hidden_dim\n",
    "    net_parameters['L'] = args.L\n",
    "    net_parameters['flag_resnet'] = args.resnet_type\n",
    "    net_parameters['dropout_rate'] = args.dropout_rate\n",
    "\n",
    "    # optimization parameters\n",
    "    opt_parameters = {}\n",
    "    opt_parameters['learning_rate'] = args.learning_rate\n",
    "    opt_parameters['max_epochs'] = args.max_epochs\n",
    "    opt_parameters['decay_rate'] = args.decay_rate\n",
    "    opt_parameters['batch_size'] = args.batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# parameters values in notebook mode\n",
    "###############\n",
    "if notebook_mode == True:\n",
    "\n",
    "    # network parameters\n",
    "    net_parameters = {}\n",
    "    net_parameters['nb_atoms'] = len(atom_dict.idx2word)\n",
    "    net_parameters['nb_bonds'] = len(bond_dict.idx2word)\n",
    "    net_parameters['max_atom_count'] = data_train[-1].N\n",
    "    net_parameters['hidden_dim'] = 200\n",
    "    net_parameters['output_dim'] = 1\n",
    "    net_parameters['L'] = 16\n",
    "    net_parameters['flag_resnet'] = 'he_resnet_2'\n",
    "    net_parameters['dropout_rate'] = 0.5\n",
    "    net_parameters['dropout_rate_1'] = 0.2\n",
    "    \n",
    "    \n",
    "    # optimization parameters\n",
    "    opt_parameters = {}\n",
    "    opt_parameters['learning_rate'] = 0.001\n",
    "    opt_parameters['max_epochs'] = 350  \n",
    "    #opt_parameters['max_epochs'] = 2 ### debug  \n",
    "    opt_parameters['decay_rate'] = 1.2\n",
    "    opt_parameters['batch_size'] = 50\n",
    "    \n",
    "    # save results\n",
    "    args = []\n",
    "    args.append(['max_nb_atoms',net_parameters['max_atom_count']])\n",
    "    args.append(['max_epochs',opt_parameters['max_epochs']])\n",
    "    args.append(['batch_size',opt_parameters['batch_size']])\n",
    "    args.append(['decay_rate',opt_parameters['decay_rate']])\n",
    "    args.append(['learning_rate',opt_parameters['learning_rate']])\n",
    "    args.append(['hidden_dim',net_parameters['hidden_dim']])\n",
    "    args.append(['L',net_parameters['L']])\n",
    "    args.append(['resnet_type',net_parameters['flag_resnet']])\n",
    "    args.append(['dropout+rate',net_parameters['dropout_rate']])\n",
    "    args.append(['dropout+rate_1',net_parameters['dropout_rate_1']])\n",
    "    args.append(['gpu_id',gpu_id])\n",
    "    args.append(['server_id',server_id])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeNet_regression(\n",
      "  (molecule_encoder): molecule_encoder(\n",
      "    (atoms_embedding): Embedding(4, 200)\n",
      "    (bonds_embedding): Embedding(5, 200)\n",
      "    (convnet_layers): ModuleList(\n",
      "      (0): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): basic_convnet_layer(\n",
      "        (node_convnet_feat): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (drop): Dropout(p=0.5)\n",
      "        (drop_e): Dropout(p=0.2)\n",
      "        (node_convnet_feat_2): node_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (edge_convnet_feat_2): edge_convnet_feat(\n",
      "          (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (bn_node_2): bn_node(\n",
      "          (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (bn_edge_2): bn_edge(\n",
      "          (bn): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (edges_to_vector): edges_to_vector(\n",
      "      (gate): edge_convnet_feat(\n",
      "        (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (W): Linear(in_features=200, out_features=200, bias=True)\n",
      "      )\n",
      "      (A): Linear(in_features=200, out_features=200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp): mlp(\n",
      "    (U): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (V): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "net_parameters: {'nb_atoms': 4, 'nb_bonds': 5, 'max_atom_count': 9, 'hidden_dim': 200, 'output_dim': 1, 'L': 16, 'flag_resnet': 'he_resnet_2', 'dropout_rate': 0.5, 'dropout_rate_1': 0.2}\n",
      "opt_parameters: {'learning_rate': 0.001, 'max_epochs': 350, 'decay_rate': 1.2, 'batch_size': 50}\n",
      "nb net parameters: 3431801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:3  epoch:0 epoch time:185s total time:0.1h lr:1.00e-03 loss:0.410/0.250 acc:9.545/5.823 \n",
      "gpu:3  epoch:1 epoch time:185s total time:0.1h lr:1.00e-03 loss:0.220/0.268 acc:5.115/6.225 \n",
      "gpu:3  epoch:2 epoch time:183s total time:0.2h lr:1.00e-03 loss:0.170/0.153 acc:3.954/3.556 \n",
      "gpu:3  epoch:3 epoch time:187s total time:0.2h lr:1.00e-03 loss:0.156/0.157 acc:3.619/3.660 \n",
      "gpu:3  epoch:4 epoch time:184s total time:0.3h lr:1.00e-03 loss:0.138/0.129 acc:3.209/3.002 \n",
      "gpu:3  epoch:5 epoch time:183s total time:0.3h lr:1.00e-03 loss:0.129/0.176 acc:3.002/4.104 \n",
      "gpu:3  epoch:6 epoch time:182s total time:0.4h lr:1.00e-03 loss:0.120/0.250 acc:2.795/5.815 \n",
      "gpu:3  epoch:7 epoch time:184s total time:0.4h lr:1.00e-03 loss:0.115/0.119 acc:2.671/2.764 \n",
      "gpu:3  epoch:8 epoch time:183s total time:0.5h lr:1.00e-03 loss:0.110/0.115 acc:2.562/2.677 \n",
      "gpu:3  epoch:9 epoch time:183s total time:0.5h lr:1.00e-03 loss:0.107/0.180 acc:2.482/4.192 \n",
      "gpu:3  epoch:10 epoch time:184s total time:0.6h lr:1.00e-03 loss:0.104/0.147 acc:2.408/3.430 \n",
      "gpu:3  epoch:11 epoch time:174s total time:0.6h lr:1.00e-03 loss:0.100/0.104 acc:2.333/2.411 \n",
      "gpu:3  epoch:12 epoch time:183s total time:0.7h lr:1.00e-03 loss:0.098/0.098 acc:2.273/2.283 \n",
      "gpu:3  epoch:13 epoch time:184s total time:0.7h lr:1.00e-03 loss:0.096/0.102 acc:2.228/2.383 \n",
      "gpu:3  epoch:14 epoch time:183s total time:0.8h lr:1.00e-03 loss:0.094/0.105 acc:2.183/2.453 \n",
      "gpu:3  epoch:15 epoch time:186s total time:0.8h lr:1.00e-03 loss:0.092/0.113 acc:2.131/2.638 \n",
      "gpu:3  epoch:16 epoch time:202s total time:0.9h lr:1.00e-03 loss:0.090/0.091 acc:2.098/2.113 \n",
      "gpu:3  epoch:17 epoch time:181s total time:0.9h lr:1.00e-03 loss:0.088/0.093 acc:2.058/2.157 \n",
      "gpu:3  epoch:18 epoch time:195s total time:1.0h lr:1.00e-03 loss:0.087/0.108 acc:2.030/2.516 \n",
      "gpu:3  epoch:19 epoch time:184s total time:1.0h lr:1.00e-03 loss:0.085/0.100 acc:1.988/2.330 \n",
      "gpu:3  epoch:20 epoch time:182s total time:1.1h lr:1.00e-03 loss:0.084/0.087 acc:1.958/2.021 \n",
      "gpu:3  epoch:21 epoch time:180s total time:1.1h lr:1.00e-03 loss:0.083/0.091 acc:1.933/2.105 \n",
      "gpu:3  epoch:22 epoch time:180s total time:1.2h lr:8.33e-04 loss:0.082/0.087 acc:1.917/2.028 \n",
      "gpu:3  epoch:23 epoch time:182s total time:1.2h lr:8.33e-04 loss:0.080/0.082 acc:1.854/1.903 \n",
      "gpu:3  epoch:24 epoch time:188s total time:1.3h lr:8.33e-04 loss:0.078/0.096 acc:1.813/2.231 \n",
      "gpu:3  epoch:25 epoch time:184s total time:1.3h lr:8.33e-04 loss:0.077/0.092 acc:1.790/2.138 \n",
      "gpu:3  epoch:26 epoch time:179s total time:1.4h lr:8.33e-04 loss:0.076/0.079 acc:1.769/1.827 \n",
      "gpu:3  epoch:27 epoch time:181s total time:1.4h lr:6.94e-04 loss:0.075/0.088 acc:1.755/2.055 \n",
      "gpu:3  epoch:28 epoch time:178s total time:1.5h lr:6.94e-04 loss:0.073/0.085 acc:1.696/1.972 \n",
      "gpu:3  epoch:29 epoch time:179s total time:1.5h lr:6.94e-04 loss:0.072/0.079 acc:1.674/1.838 \n",
      "gpu:3  epoch:30 epoch time:180s total time:1.6h lr:6.94e-04 loss:0.071/0.084 acc:1.648/1.943 \n",
      "gpu:3  epoch:31 epoch time:180s total time:1.6h lr:5.79e-04 loss:0.071/0.085 acc:1.640/1.970 \n",
      "gpu:3  epoch:32 epoch time:180s total time:1.7h lr:5.79e-04 loss:0.068/0.078 acc:1.588/1.807 \n",
      "gpu:3  epoch:33 epoch time:180s total time:1.7h lr:5.79e-04 loss:0.067/0.079 acc:1.569/1.836 \n",
      "gpu:3  epoch:34 epoch time:180s total time:1.8h lr:4.82e-04 loss:0.067/0.076 acc:1.560/1.762 \n",
      "gpu:3  epoch:35 epoch time:182s total time:1.8h lr:4.82e-04 loss:0.065/0.070 acc:1.517/1.624 \n",
      "gpu:3  epoch:36 epoch time:182s total time:1.9h lr:4.82e-04 loss:0.064/0.081 acc:1.500/1.888 \n",
      "gpu:3  epoch:37 epoch time:188s total time:1.9h lr:4.02e-04 loss:0.064/0.084 acc:1.490/1.953 \n",
      "gpu:3  epoch:38 epoch time:184s total time:2.0h lr:4.02e-04 loss:0.063/0.069 acc:1.454/1.598 \n",
      "gpu:3  epoch:39 epoch time:187s total time:2.0h lr:4.02e-04 loss:0.062/0.073 acc:1.436/1.690 \n",
      "gpu:3  epoch:40 epoch time:188s total time:2.1h lr:3.35e-04 loss:0.061/0.069 acc:1.426/1.613 \n",
      "gpu:3  epoch:41 epoch time:211s total time:2.1h lr:3.35e-04 loss:0.060/0.071 acc:1.399/1.662 \n",
      "gpu:3  epoch:42 epoch time:187s total time:2.2h lr:3.35e-04 loss:0.059/0.071 acc:1.382/1.652 \n",
      "gpu:3  epoch:43 epoch time:190s total time:2.3h lr:2.79e-04 loss:0.059/0.070 acc:1.370/1.631 \n",
      "gpu:3  epoch:44 epoch time:200s total time:2.3h lr:2.79e-04 loss:0.058/0.068 acc:1.345/1.585 \n",
      "gpu:3  epoch:45 epoch time:180s total time:2.4h lr:2.33e-04 loss:0.057/0.070 acc:1.337/1.631 \n",
      "gpu:3  epoch:46 epoch time:180s total time:2.4h lr:2.33e-04 loss:0.056/0.065 acc:1.313/1.520 \n",
      "gpu:3  epoch:47 epoch time:180s total time:2.5h lr:1.94e-04 loss:0.056/0.066 acc:1.301/1.534 \n",
      "gpu:3  epoch:48 epoch time:180s total time:2.5h lr:1.94e-04 loss:0.055/0.066 acc:1.282/1.531 \n",
      "gpu:3  epoch:49 epoch time:182s total time:2.6h lr:1.62e-04 loss:0.055/0.065 acc:1.274/1.505 \n",
      "gpu:3  epoch:50 epoch time:209s total time:2.6h lr:1.62e-04 loss:0.054/0.065 acc:1.257/1.521 \n",
      "gpu:3  epoch:51 epoch time:186s total time:2.7h lr:1.35e-04 loss:0.054/0.064 acc:1.248/1.482 \n",
      "gpu:3  epoch:52 epoch time:180s total time:2.7h lr:1.12e-04 loss:0.053/0.064 acc:1.237/1.489 \n",
      "gpu:3  epoch:53 epoch time:181s total time:2.8h lr:1.12e-04 loss:0.052/0.064 acc:1.219/1.495 \n",
      "gpu:3  epoch:54 epoch time:184s total time:2.8h lr:9.35e-05 loss:0.052/0.064 acc:1.211/1.492 \n",
      "gpu:3  epoch:55 epoch time:181s total time:2.9h lr:7.79e-05 loss:0.052/0.064 acc:1.204/1.487 \n",
      "gpu:3  epoch:56 epoch time:182s total time:2.9h lr:6.49e-05 loss:0.051/0.064 acc:1.196/1.487 \n",
      "gpu:3  epoch:57 epoch time:190s total time:3.0h lr:5.41e-05 loss:0.051/0.064 acc:1.185/1.486 \n",
      "gpu:3  epoch:58 epoch time:183s total time:3.0h lr:4.51e-05 loss:0.051/0.063 acc:1.181/1.464 \n",
      "gpu:3  epoch:59 epoch time:184s total time:3.1h lr:3.76e-05 loss:0.051/0.063 acc:1.177/1.476 \n",
      "gpu:3  epoch:60 epoch time:185s total time:3.1h lr:3.13e-05 loss:0.050/0.063 acc:1.170/1.460 \n",
      "gpu:3  epoch:61 epoch time:180s total time:3.2h lr:2.61e-05 loss:0.050/0.063 acc:1.165/1.463 \n",
      "gpu:3  epoch:62 epoch time:180s total time:3.2h lr:2.17e-05 loss:0.050/0.064 acc:1.163/1.482 \n",
      "gpu:3  epoch:63 epoch time:179s total time:3.3h lr:1.81e-05 loss:0.050/0.063 acc:1.162/1.473 \n",
      "gpu:3  epoch:64 epoch time:212s total time:3.3h lr:1.51e-05 loss:0.050/0.063 acc:1.158/1.460 \n",
      "gpu:3  epoch:65 epoch time:180s total time:3.4h lr:1.26e-05 loss:0.050/0.063 acc:1.153/1.459 \n",
      "gpu:3  epoch:66 epoch time:214s total time:3.4h lr:1.05e-05 loss:0.050/0.063 acc:1.153/1.459 \n",
      "gpu:3  epoch:67 epoch time:218s total time:3.5h lr:8.74e-06 loss:0.049/0.064 acc:1.150/1.478 \n",
      "gpu:3  epoch:68 epoch time:180s total time:3.6h lr:7.28e-06 loss:0.049/0.063 acc:1.151/1.462 \n",
      "gpu:3  epoch:69 epoch time:180s total time:3.6h lr:6.07e-06 loss:0.050/0.063 acc:1.151/1.455 \n",
      "gpu:3  epoch:70 epoch time:180s total time:3.7h lr:5.06e-06 loss:0.049/0.063 acc:1.146/1.457 \n",
      "gpu:3  epoch:71 epoch time:181s total time:3.7h lr:4.21e-06 loss:0.049/0.063 acc:1.146/1.460 \n",
      "gpu:3  epoch:72 epoch time:179s total time:3.8h lr:3.51e-06 loss:0.049/0.063 acc:1.148/1.458 \n",
      "gpu:3  epoch:73 epoch time:180s total time:3.8h lr:2.93e-06 loss:0.049/0.063 acc:1.147/1.458 \n",
      "gpu:3  epoch:74 epoch time:194s total time:3.9h lr:2.44e-06 loss:0.049/0.063 acc:1.148/1.460 \n",
      "gpu:3  epoch:75 epoch time:188s total time:3.9h lr:2.03e-06 loss:0.049/0.063 acc:1.147/1.457 \n",
      "gpu:3  epoch:76 epoch time:180s total time:4.0h lr:1.69e-06 loss:0.049/0.064 acc:1.145/1.493 \n",
      "gpu:3  epoch:77 epoch time:188s total time:4.0h lr:1.41e-06 loss:0.049/0.063 acc:1.148/1.461 \n",
      "gpu:3  epoch:78 epoch time:221s total time:4.1h lr:1.18e-06 loss:0.049/0.063 acc:1.145/1.460 \n",
      "gpu:3  epoch:79 epoch time:220s total time:4.1h lr:9.80e-07 loss:0.049/0.063 acc:1.145/1.457 \n",
      "gpu:3  epoch:80 epoch time:193s total time:4.2h lr:8.16e-07 loss:0.049/0.063 acc:1.143/1.461 \n",
      "gpu:3  epoch:81 epoch time:179s total time:4.2h lr:6.80e-07 loss:0.049/0.063 acc:1.148/1.459 \n",
      "gpu:3  epoch:82 epoch time:187s total time:4.3h lr:5.67e-07 loss:0.049/0.063 acc:1.144/1.457 \n",
      "gpu:3  epoch:83 epoch time:180s total time:4.3h lr:4.72e-07 loss:0.049/0.063 acc:1.145/1.456 \n",
      "gpu:3  epoch:84 epoch time:180s total time:4.4h lr:3.94e-07 loss:0.049/0.063 acc:1.146/1.458 \n",
      "gpu:3  epoch:85 epoch time:185s total time:4.4h lr:3.28e-07 loss:0.049/0.063 acc:1.143/1.464 \n",
      "gpu:3  epoch:86 epoch time:187s total time:4.5h lr:2.73e-07 loss:0.049/0.063 acc:1.143/1.459 \n",
      "gpu:3  epoch:87 epoch time:187s total time:4.5h lr:2.28e-07 loss:0.049/0.063 acc:1.144/1.459 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:3  epoch:88 epoch time:208s total time:4.6h lr:1.90e-07 loss:0.049/0.063 acc:1.146/1.476 \n",
      "gpu:3  epoch:89 epoch time:186s total time:4.7h lr:1.58e-07 loss:0.049/0.063 acc:1.145/1.459 \n",
      "gpu:3  epoch:90 epoch time:186s total time:4.7h lr:1.32e-07 loss:0.049/0.063 acc:1.144/1.468 \n",
      "gpu:3  epoch:91 epoch time:193s total time:4.8h lr:1.10e-07 loss:0.049/0.063 acc:1.143/1.465 \n",
      "gpu:3  epoch:92 epoch time:209s total time:4.8h lr:9.16e-08 loss:0.049/0.063 acc:1.146/1.460 \n",
      "gpu:3  epoch:93 epoch time:180s total time:4.9h lr:7.63e-08 loss:0.049/0.066 acc:1.143/1.535 \n",
      "gpu:3  epoch:94 epoch time:180s total time:4.9h lr:6.36e-08 loss:0.049/0.063 acc:1.141/1.463 \n",
      "gpu:3  epoch:95 epoch time:211s total time:5.0h lr:5.30e-08 loss:0.049/0.063 acc:1.145/1.460 \n",
      "gpu:3  epoch:96 epoch time:184s total time:5.0h lr:4.42e-08 loss:0.049/0.063 acc:1.144/1.457 \n",
      "gpu:3  epoch:97 epoch time:180s total time:5.1h lr:3.68e-08 loss:0.049/0.063 acc:1.145/1.455 \n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    main function\n",
    "    \"\"\"\n",
    "    \n",
    "    # save results in a .txt file\n",
    "    time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "    file_name = 'logs'+'/'+time_stamp + \"-server{}\".format(server_id) + \"-gpu{}\".format(gpu_id) + \".txt\"\n",
    "    file = open(file_name,\"w\",1) \n",
    "    file.write(time_stamp+'\\n\\n') \n",
    "    mystr = \"QM9 regression with all molecules v1\"\n",
    "    file.write(mystr+'\\n\\n')\n",
    "    if notebook_mode == False:\n",
    "        for arg in vars(args):\n",
    "            file.write(arg)\n",
    "            val=\"={}\".format(getattr(args, arg))\n",
    "            file.write(val)\n",
    "            file.write('\\n')\n",
    "        file.write('\\n\\n') \n",
    "    if notebook_mode == True:\n",
    "        for _,arg in enumerate(args):\n",
    "            val=\"{arg0}={arg1}\".format(arg0=arg[0],arg1=arg[1])\n",
    "            file.write(val)\n",
    "            file.write('\\n')\n",
    "        file.write('\\n\\n') \n",
    "            \n",
    "    # instantiate the network\n",
    "    net = MoleculeNet_regression(net_parameters)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "    \n",
    "    # number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('net_parameters:',net_parameters)\n",
    "    print('opt_parameters:',opt_parameters)\n",
    "    print('nb net parameters:',nb_param)\n",
    "    \n",
    "    # train parameters\n",
    "    train_loss_old = 1e6\n",
    "    lr = opt_parameters['learning_rate']\n",
    "    decay_rate = opt_parameters['decay_rate']\n",
    "    optimizer = net.update(lr) \n",
    "    max_epochs = opt_parameters['max_epochs']\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = net.update(lr) \n",
    "\n",
    "    # loop over epochs\n",
    "    start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        start_epoch = time.time()\n",
    "        \n",
    "        # train, test\n",
    "        train_loss, train_acc = train_one_epoch(net, optimizer, opt_parameters)\n",
    "        test_loss, test_acc = evaluate_test_set(net, opt_parameters)\n",
    "        \n",
    "        # update learning rate \n",
    "        if train_loss > 0.99* train_loss_old:\n",
    "            lr /= decay_rate\n",
    "        optimizer = net.update_learning_rate(optimizer, lr)\n",
    "        train_loss_old = train_loss\n",
    "        \n",
    "        # print intermediate results\n",
    "        print_one_epoch_result = ( 'gpu:{ID}  epoch:{EP} epoch time:{epoch_time:.0f}s total time:{fromstart:.1f}h '\n",
    "                   'lr:{LR:.2e} '\n",
    "                   'loss:{train_loss:.3f}/{test_loss:.3f} '\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    ID=gpu_id,\n",
    "                    EP=epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    LR= lr,  \n",
    "                    train_loss=train_loss, test_loss=test_loss,\n",
    "                    train_acc=train_acc, test_acc=test_acc ) )\n",
    "        print(print_one_epoch_result)\n",
    "        \n",
    "        # save intermediate results\n",
    "        save_one_epoch_result = ( 'server/gpu {IDs}/{ID}  epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    IDs=server_id,\n",
    "                    ID=gpu_id,\n",
    "                    EP=epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    LR= lr,  \n",
    "                    train_loss=train_loss, test_loss=test_loss,\n",
    "                    train_acc=train_acc, test_acc=test_acc ) )\n",
    "        file.write(save_one_epoch_result+'\\n')\n",
    "\n",
    "        \n",
    "    # save final result\n",
    "    result = {}\n",
    "    result['max_epochs'] = opt_parameters['max_epochs']\n",
    "    result['batch_size'] = opt_parameters['batch_size']\n",
    "    result['decay_rate'] = opt_parameters['decay_rate']\n",
    "    result['learning_rate'] = opt_parameters['learning_rate']\n",
    "    result['final_learning_rate'] = lr\n",
    "    result['hidden_dim'] = net_parameters['hidden_dim']\n",
    "    result['L'] = net_parameters['L']\n",
    "    result['gpu_id'] = gpu_id\n",
    "    result['server_id'] = server_id\n",
    "    result['train_loss'] = train_loss\n",
    "    result['test_loss'] = test_loss\n",
    "    result['train_acc'] = train_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    result['total_time'] = (time.time()-start)/3600\n",
    "    result['epoch_time'] = time.time()-start_epoch\n",
    "    file_name = 'logs'+'/'+time_stamp + \"-server{}\".format(server_id) + \"-gpu{}\".format(gpu_id) + \".pickle\"\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(result, handle) \n",
    "\n",
    "           \n",
    "main(args)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
